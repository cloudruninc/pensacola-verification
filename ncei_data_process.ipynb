{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<div style=\"width:1000 px\">\n",
    "\n",
    "<div style=\"float:right; width:140 px; height:140px;\">\n",
    "<img src=\"https://cloudrun.co/img/logo_noname.png\" alt=\"Cloudrun Logo\" style=\"height: 140px;\">\n",
    "</div>\n",
    "\n",
    "<h1>Step 2: Pre-process Downloaded NCEI GFS+NAM Data from Step 1</h1>\n",
    "<h2>By Kayla Besong</h2>\n",
    "This program is specific to Pensecola Airport Meteorological data, generating netcdf files from previously downloaded GFS/NAM GRIB with the closest latitude/longitude point and variables with names that directly match those used in observation outputs. The resulting netcdfs will be read in step 3, where direct comparision of variables for both NCEP models, Cloudrun output, and observation will take place. Because this is specific to Pensecola Airport, this notebook would have to be adjusted only in the 'xr_retrun_pensecola' function to adjust to other locations and edit variables. If other variables are desried, the typeOfLevel and cfVarName will need to be added. A list of variables/level can be accessed by clicking on the .inv file in the url directory where the file was downloaded from in step 1. The variables will then need to be added to 'xr_retrun_pensecola' as an xarray variable and the ncep_vars dictionary input below. \n",
    "\n",
    "\n",
    "<div style=\"clear:both\"></div>\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "import matplotlib.dates as mdates\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neci_grab_variables(ncep_var_dict, directory):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function utilizes dictionaries and the ncei_grib_reading function to loop through each run and extract multiple variables with the type of level as the key and the variable as the value. \n",
    "    This is to iterate over all desired variables be stored in one dictionary for later processing into array based on run. The reasoning is that to open using cfgrib engine, the type of \n",
    "    level and variable needs to be specified and only one variable can be opened for a given run at a time. The approach is to consider a variable and open all the files/runs and store them \n",
    "    in a new dictionary. The resulting dictrionary will have the same keys, but the values will now be a list of xarrays with one variable with the length equal to the amount of runs. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    new_dict = {}                                               # generate empty dictionary for given run \n",
    "    \n",
    "\n",
    "    for key,value in ncep_var_dict.items():\n",
    "\n",
    "        if type(value) == list:\n",
    "            for i in range(len(value)):\n",
    "                \n",
    "                new_key = key + '_' + value[i]\n",
    "                \n",
    "                if value[i] == 'prate' or key == 'unknown':        # surface and unknown levels require stepType instant or avg \n",
    "                    \n",
    "                    new_dict[key] = ncei_grib_reading(directory, key, value[i], stepType = 'instant')        # call ncei reading to loop through directory and store \n",
    "                \n",
    "                elif value[i] == 'tp':\n",
    "                    \n",
    "                    new_dict[key] = ncei_grib_reading(directory, key, value[i], stepType = 'accum')        # call ncei reading to loop through directory and store \n",
    "\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    new_dict[new_key] = ncei_grib_reading(directory, key, value[i])\n",
    "\n",
    "        else:\n",
    "            \n",
    "            if value == 'prate': \n",
    "                \n",
    "                new_dict[key] = ncei_grib_reading(directory, key, value, stepType = 'instant')\n",
    "            \n",
    "            elif value == 'tp':\n",
    "                \n",
    "                new_dict[key] = ncei_grib_reading(directory, key, value, stepType = 'accum')\n",
    "\n",
    "            else:\n",
    "                \n",
    "                new_dict[key] = ncei_grib_reading(directory, key, value)\n",
    "        \n",
    "            \n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncei_grib_reading(directory, typeOfLevel, cfVarName, stepType = None):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function loops through the directories/subdirectories established in step 1 that conatin the downloaded GFS and NAM data. Each file in files loop takes each run, \n",
    "    opens the variables requested, then concatenates along the forecast period (72 for this example, total of 25 valid_times). \n",
    "    The result is an array of xrarry dataarrays that contain the runs with one variable.\n",
    "    \n",
    "    '''   \n",
    "\n",
    "    \n",
    "    collection_of_runs = []                                                                                               ## empty array to store inddividual runs \n",
    "    \n",
    "    for root, dirs, files in sorted(os.walk(directory)):                                                                  ## navigate os\n",
    "\n",
    "        print('reading ' + root + ' variable ' + typeOfLevel + ' '+ cfVarName)                                            ## ensure correct directory, status\n",
    "\n",
    "        datas = []\n",
    "        \n",
    "        for file in sorted(files):\n",
    "\n",
    "\n",
    "            if file[-4:] == 'grb2':                                           ## error handle, read only netcdf \n",
    "                \n",
    "                \n",
    "                ## the next set of conidtions were required in specifying levels of detail needed for xarray to handle the grib file. If too many or too few filter_by_keys inputs are present, the opening of the fle will not be achieved \n",
    "                \n",
    "                \n",
    "                if typeOfLevel == 'surface' and stepType == 'instant':\n",
    "                    \n",
    "                    datas.append(xr.open_dataset(root+'/'+file, engine = 'cfgrib',  backend_kwargs = dict(filter_by_keys={'typeOfLevel': typeOfLevel, 'cfVarName': cfVarName, 'stepType': stepType}))[cfVarName])                 \n",
    "                                    \n",
    "                elif typeOfLevel == 'surface' and stepType == 'accum':\n",
    "                    \n",
    "                    datas.append(xr.open_dataset(root+'/'+file, engine = 'cfgrib',  backend_kwargs = dict(filter_by_keys={'typeOfLevel': typeOfLevel, 'stepType': stepType}))[cfVarName])                 \n",
    "                \n",
    "                elif typeOfLevel == 'heightAboveGround':\n",
    "                    \n",
    "                    datas.append(xr.open_dataset(root+'/'+file, engine = 'cfgrib',  backend_kwargs = dict(filter_by_keys={'typeOfLevel': typeOfLevel, 'cfVarName': cfVarName}))[cfVarName])                 \n",
    "                \n",
    "                elif typeOfLevel == 'meanSea':\n",
    "\n",
    "                    datas.append(xr.open_dataset(root+'/'+file, engine = 'cfgrib',  backend_kwargs = dict(filter_by_keys={'typeOfLevel': typeOfLevel}))[cfVarName])\n",
    "\n",
    "                else:\n",
    "                    print('has !!!!!!NOT!!!!! appended ' + root + ' variable ' + typeOfLevel + ' '+ cfVarName)\n",
    "                    \n",
    "                                                                                                                                     \n",
    "            else:\n",
    "\n",
    "                continue\n",
    "        \n",
    "        if len(datas) > 0:                                                                   ## make sure non empty list of data files are being appended\n",
    "                        \n",
    "            collection_of_runs.append(xr.concat(datas, dim = 'valid_time'))                  ## concatinatinate dataset for individual run set \n",
    "\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return collection_of_runs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_retrun_pensecola(new_dict, directory):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes the output of ncei_grib_reading, an array of xrarry dataarrays that contain all runs for one variable, and turns it into one dataset per run for all variables with unit conversions.\n",
    "    This step also finds the nearest gridpoint from the model to the PENSACOLA AIRPORT specifically lat+lon point. These variables are also specific to PENSACOLA Airport station data\n",
    "    variable names. This is so that when post processing, the same titles can be used to select all data i.e. this function homogenizes all model data into the same format as obs, units included. \n",
    "    The output is an array of xarrays with all variables for for a given run (verses a sweet of datararrays for all runs with one variable) to be exported as netcdf files. \n",
    "    \n",
    "    '''  \n",
    "    \n",
    "    \n",
    "    new_dict_keys = list(new_dict.keys())\n",
    "    \n",
    "    ## Pensecola Aitport gridpoint. Can become an input variable of this function later on for other locations. \n",
    "\n",
    "    lat = 30.47806\n",
    "    lon = 360.-87.18694\n",
    "\n",
    "    \n",
    "    ## locating the neartest point to lat/lon above\n",
    "    \n",
    "    abslat = np.abs(new_dict[new_dict_keys[0]][0].latitude-lat)\n",
    "    abslon = np.abs(new_dict[new_dict_keys[0]][0].longitude-lon)\n",
    "    c = np.maximum(abslon, abslat)\n",
    "\n",
    "    loc = np.where(c == np.min(c))\n",
    "\n",
    "    \n",
    "    ## the x and y cords between GFS and NAM seem to be opposite when the above procedure is used to determine nearest grid point\n",
    "    ## hence the condition below to use as selection points for x-array datasets below.\n",
    "    \n",
    "    \n",
    "    if directory[0:3] == 'GFS':\n",
    "        \n",
    "        x_sel = loc[0][0]               # 'true' x\n",
    "        y_sel = loc[1][0]               # 'true' y\n",
    "        \n",
    "    elif directory[0:3] == 'NAM':\n",
    "        \n",
    "        x_sel = loc[1][0]               # 'true' y\n",
    "        y_sel = loc[0][0]               # 'true' x\n",
    "        \n",
    "\n",
    "\n",
    "    file_lab = [i for i in sorted(os.listdir(directory)) if len(i) == 10]            # ensure directory == directory developed in Step 1 ~ ncei_data_grab.ipynb, the len of this directory = len of collection of runs = length needing to be looped through. Used as an error handling measure. \n",
    "    array_out = []\n",
    "\n",
    "    \n",
    "    \n",
    "    ## loop through each collection of runs and extract each variable for that run, convert as needed, select nearest grid point to obs and convert \n",
    "\n",
    "    for j in range(len(file_lab)):\n",
    "        \n",
    "\n",
    "        wind_speed = np.sqrt(new_dict['heightAboveGround_u10'][j][:].isel(longitude = x_sel, latitude = y_sel)**2 + new_dict['heightAboveGround_v10'][j][:].isel(longitude = x_sel, latitude = y_sel)**2)\n",
    "        wind_dir1 = np.arctan2(-new_dict['heightAboveGround_u10'][j][:].isel(longitude = x_sel, latitude = y_sel).values, -new_dict['heightAboveGround_v10'][j][:].isel(longitude = x_sel, latitude = y_sel))*(-180./np.pi)\n",
    "        wind_dir1[np.where(wind_dir1 < 0)] += 360\n",
    "\n",
    "        array_out.append(xr.Dataset({'wind_speed': (['time'], wind_speed),\n",
    "                                   'wind_dir': (['time'], wind_dir1),\n",
    "                                   'air_pressure_at_mean_sea_level': (['time'], new_dict['meanSea'][j][:].isel(longitude = x_sel, latitude = y_sel)/100.),\n",
    "                                   'air_temperature_2m': (['time'], new_dict['heightAboveGround_t2m'][j][:].isel(longitude = x_sel, latitude = y_sel)-273.15),\n",
    "                                   'relative_humidity_2m': (['time'], new_dict['heightAboveGround_r2'][j][:].isel(longitude = x_sel, latitude = y_sel)),\n",
    "                                   'rainfall_amount': (['time'], new_dict['surface'][j][:].isel(longitude = x_sel, latitude = y_sel))},\n",
    "\n",
    "                                    coords = {'time': sorted(new_dict['meanSea'][j][:].valid_time.values)}))\n",
    "\n",
    "\n",
    "    return array_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_nc(array_out, directory):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes the output of xr_retrun_pensecola, array xarray Datasets with all variables for for a given run and exports each run with all desired variables to a netcdf file. \n",
    "    \n",
    "    ''' \n",
    "    \n",
    "    file_lab = [i for i in sorted(os.listdir(directory)) if len(i) == 10]\n",
    "    main_dir = 'processed_ncei'\n",
    "    sub_dir = directory[0:3] \n",
    "   \n",
    "    try: os.makedirs(os.path.join(main_dir,sub_dir))\n",
    "    except OSError: print('Directory already exists for ' + main_dir + 'and ' + sub_dir)  ## let it be known if this directory for model + year_mon_day already exists\n",
    "    \n",
    "    paths = [main_dir + '/' + sub_dir + '/' + directory[0:3] +'_%s.nc' %  ymdt for ymdt in file_lab]\n",
    "    \n",
    "    for array, path in zip(array_out, paths):\n",
    "        array.to_netcdf(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meanSea': 'mslet',\n",
       " 'surface': 'prate',\n",
       " 'heightAboveGround': ['t2m', 'r2', 'u10', 'v10']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncep_var_gfs = {'meanSea': 'mslet', 'surface': 'prate', 'heightAboveGround': ['t2m', 'r2', 'u10', 'v10']}\n",
    "ncep_var_gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "out_to_nc(xr_retrun_pensecola(neci_grab_variables(ncep_var_gfs, \"GFS_data\"), 'GFS_data'), \"GFS_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meanSea': 'mslet',\n",
       " 'surface': 'tp',\n",
       " 'heightAboveGround': ['t2m', 'r2', 'u10', 'v10']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncep_var_nam = {'meanSea': 'mslet', 'surface': 'tp', 'heightAboveGround': ['t2m', 'r2', 'u10', 'v10']}\n",
    "ncep_var_nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "out_to_nc(xr_retrun_pensecola(neci_grab_variables(ncep_var_nam, \"NAM_data\"), \"NAM_data\"), 'NAM_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
